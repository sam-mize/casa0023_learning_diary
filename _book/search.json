[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023: Learning Diary",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Persson, Magnus, Eva Lindberg, and Heather Reese. 2018. “Tree\nSpecies Classification with Multi-Temporal Sentinel-2\nData.” Remote Sensing 10 (11): 1794. https://doi.org/10.3390/rs10111794.\n\n\nSun, Tong, Jianbo Qi, and Huaguo Huang. 2020. “Discovering Forest\nHeight Changes Based on Spaceborne Lidar Data of ICESat-1\nin 2005 and ICESat-2 in 2019: A Case Study in the\nBeijing-Tianjin-Hebei Region of China.”\nForest Ecosystems 7 (1): 53. https://doi.org/10.1186/s40663-020-00265-w.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#casa0023-remote-sensing-cities-and-environments",
    "href": "index.html#casa0023-remote-sensing-cities-and-environments",
    "title": "CASA0023: Learning Diary",
    "section": "CASA0023: Remote Sensing Cities and Environments",
    "text": "CASA0023: Remote Sensing Cities and Environments\nIt’s module number 23 in CASA, and it’s subject is remote sensing cities and environments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sam-mize",
    "href": "index.html#sam-mize",
    "title": "CASA0023: Learning Diary",
    "section": "Sam Mize",
    "text": "Sam Mize\nI’m a student learning about remotely sensing cities and environments in CASA0023.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#casa0023-remotely-sensing-cities-and-environments",
    "href": "index.html#casa0023-remotely-sensing-cities-and-environments",
    "title": "CASA0023: Learning Diary",
    "section": "CASA0023: Remotely Sensing Cities and Environments",
    "text": "CASA0023: Remotely Sensing Cities and Environments\nIt’s module number 0023 in CASA, and its subject is remotely sensing cities and environments.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  January 13, 2026",
    "section": "",
    "text": "2 Summary\nRemote Sensing (since we’re mostly remotely sensing Earth, used interchangeably with “Earth Observation”) is acquisition of information from a distance using sensors. Typically, we’ll be working with data from satellites, although planes, drones, and anything else with any kind of sensor could potentially be used. Lidar data is point clouds, but we’ll typically be working with raster data. Even if we do use Lidar, it can usually be converted to a raster heightmap. A variety of observation satellites provide raster data with varying resolutions analogous to more typical video data:\n\nSpatial Resolution (10~60m depending on source and spectral band) ≈ video resolution\nSpectral Resolution (visible plus longer and sometimes shorter bands) ≈ color channels\nTemporal Resolution (1~16 days for satellite) ≈ frame rate\nRadiometric Resolution (4~12 bits maybe) ≈ bit depth\n\nEarth’s atmosphere interferes differently with transmission of different wavelengths of light. Typically, we’ll be working with data which has been processed to control for those effects and “reflect” the surface of Earth as if the atmosphere was perfectly transparent to all wavelengths. Sometimes things that appear similar to our eyes affect light outside the visible spectrum very differently, allowing them to be easily distinguished by a specific spectral band. In other cases more subtle differences in a variety of frequency bands (a “spectral signature”), analogous to a frequency-response in acoustics, circuit analysis, vibration analysis, or controls engineering, might distinguish them. A trivial example presented in the introductory lecture and practical is that foliage appears very bright in the near infrared (NIR) band, but dark in blue and green.\n\nIn the lecture, it was recommended to use multiple tiles from time-adjacent passes and take the median value for each pixel. Why? If you’re working with 4-bit radiometric resolution and you have [B1111, B1111, B1111, B1111, B1001, B1000, B1000], you don’t want B1111 = 15 unless you’re studying clouds, and why use B1000 = 8 when you can use 8.33? Presumably everything is getting converted to floating point arrays anyway for analysis?\n\n\n3 Applications\nPersson, Lindberg, and Reese (2018) trained a random forest model to predict specific tree species using various sets of frequency bands available from Sentinel-2 to narrow down which specific bands are most important for that task in the context of central Sweden. They found that the set of 13 spectral bands provided reasonably good accuracy, but stressed the importance of using imagery from the seasons at which the foliage is most distinguishable, in the early spring, late spring, and fall. Just as mentioned in the intro lecture, the researchers derived spectral signatures characteristic of each species of interest. The results here are interesting and maybe useful in Sweden or other places where large areas of forests are mostly the same species. North America, especially east of the Rocky Mountains has very mixed forests which would likely not be distinguishable without better spatial resolution. \n[This](https://www.github.com/sentinel-hub/natural-color)[https://www.github.com/sentinel-hub/natural-color[https://www.github.com/sentinel-hub/natural-color)) contains some examples and scripts developed to transform Sentinel color channels into more natural (to human perception) sRGB channels. This could be useful for presentations, because I’ve noticed the odd coloration makes the satellite images feel a bit detached from reality to me, like an over-saturated game or something. I imagine presentations featuring what a person feels like they would perceive if they were personally flying high overhead are more psychologically affecting, whether strictly scientifically relevant or not.\n\n\n\n\n4 Reflection\nMy brother is a forester, responsible for evaluating forest tracts for specific purposes: timber value, suitable habitat for specific wildlife, and most recently for forestry-based carbon credits. I think his industry could use more and better EO than what they currently do. For now, remote sensing is not directly applied to the tracts for which carbon credits are issued. Instead foresters directly measure every tree within randomly selected plots of the tracts. Because that’s expensive, it’s only done once every 5 years. The owners of the land are supposed to be paid for their carbon sequestration every year, so the administrators of the credits rely on predicted growth rates for the in-between years. Accumulated error in the prediction has to be made-up when the 5-year cycle ends. Maybe using EO data and a prediction model that would update every time the satellite passed by (or at least more frequently than every 5 years) could reduce the error and enable more consistent and reliable payments. \n\n\n\n\nPersson, Magnus, Eva Lindberg, and Heather Reese. 2018. “Tree Species Classification with Multi-Temporal Sentinel-2 Data.” Remote Sensing 10 (11): 1794. https://doi.org/10.3390/rs10111794.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>January 13, 2026</span>"
    ]
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Week 1",
    "section": "",
    "text": "Spatial Resolution (10~60m depending on source and spectral band) ≈ video resolution\nSpectral Resolution (visible plus longer and sometimes shorter bands) ≈ color channels\nTemporal Resolution (1~16 days for satellite) ≈ frame rate\nRadiometric Resolution (4~12 bits maybe) ≈ bit depth",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  Week 1",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nPersson, Lindberg, and Reese (2018) trained a random forest model to predict specific tree species using various sets of frequency bands available from Sentinel-2 to narrow down which specific bands are most important for that task in the context of central Sweden. They found that the set of 13 spectral bands provided reasonably good accuracy, but stressed the importance of using imagery from the seasons at which the foliage is most distinguishable, in the early spring, late spring, and fall. Just as mentioned in the intro lecture, the researchers derived spectral signatures characteristic of each species of interest. The results here are interesting and maybe useful in Sweden or other places where large areas of forests are mostly the same species. North America, especially east of the Rocky Mountains has very mixed forests which would likely not be distinguishable without better spatial resolution. \n[This](https://www.github.com/sentinel-hub/natural-color)[https://www.github.com/sentinel-hub/natural-color[https://www.github.com/sentinel-hub/natural-color)) contains some examples and scripts developed to transform Sentinel color channels into more natural (to human perception) sRGB channels. This could be useful for presentations, because I’ve noticed the odd coloration makes the satellite images feel a bit detached from reality to me, like an over-saturated game or something. I imagine presentations featuring what a person feels like they would perceive if they were personally flying high overhead are more psychologically affecting, whether strictly scientifically relevant or not.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Week 1",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy brother is a forester, responsible for evaluating forest tracts for specific purposes: timber value, suitable habitat for specific wildlife, and most recently for forestry-based carbon credits. I think his industry could use more and better EO than what they currently do. For now, remote sensing is not directly applied to the tracts for which carbon credits are issued. Instead foresters directly measure every tree within randomly selected plots of the tracts. Because that’s expensive, it’s only done once every 5 years. The owners of the land are supposed to be paid for their carbon sequestration every year, so the administrators of the credits rely on predicted growth rates for the in-between years. Accumulated error in the prediction has to be made-up when the 5-year cycle ends. Maybe using EO data and a prediction model that would update every time the satellite passed by (or at least more frequently than every 5 years) could reduce the error and enable more consistent and reliable payments. \n\n\n\n\nPersson, Magnus, Eva Lindberg, and Heather Reese. 2018. “Tree Species Classification with Multi-Temporal Sentinel-2 Data.” Remote Sensing 10 (11): 1794. https://doi.org/10.3390/rs10111794.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  January 20th, 2026",
    "section": "",
    "text": "3 Summary\nNASA’s Ice, Cloud, and Land Elevation Satellite (ICESat) launched in 2002 and collected data using its Geoscience Laser Altimeter System (GLAS) instrumentation between 2003 and 2010, when it ceased functioning. Its successor, ICESat-2, has replaced it since 2018. Fundamentally, both measure the distance from the satellite instrumentation to the Earth’s surface by shooting pulses of laser light at the Earth and recording the time required for the reflection of the laser to reach a telescope mounted parallel to the laser. On-board processing is used to detect the pulses approximately and select the regions of data to transmit to the ground for more refined processing. ICESat could only handle 40 pulses per second, but ICESat-2 can do 10000. Consequently, ICESat-2 collects 250 times more points along the same path length. The original could measure with about 10 cm resolution, and the current resolution is about 5 mm.\nxaringanExtra::embed_xaringan(url = \"https://sam-mize.github.io/casa0023/slideshow.html#1\",\n                              ratio = \"16:9\")\n\n\n4 Applications\nSun, Qi, and Huang (2020) found that combining data across both ICESat missions enabled them to estimate reforestation in remote area of China, which is important knowledge for carbon balance and general environmental science.\n\n\n5 Reflection\nI have use similar devices (in priciple, not precision or expense) and can easily see how having one in space would be useful. The higher precision and resolution of ICESat-2 has enabled more use in forestry on shorter timescales in addition to the primary ice-tracking goal of the mission. Maybe another 250-fold increase in resolution is right around the corner.\n\n\n\n\nSun, Tong, Jianbo Qi, and Huaguo Huang. 2020. “Discovering Forest Height Changes Based on Spaceborne Lidar Data of ICESat-1 in 2005 and ICESat-2 in 2019: A Case Study in the Beijing-Tianjin-Hebei Region of China.” Forest Ecosystems 7 (1): 53. https://doi.org/10.1186/s40663-020-00265-w.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>January 20th, 2026</span>"
    ]
  }
]